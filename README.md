# Fairness-Aware Post-Processing Tasks

This repository provides the implementation for the ensemble-based post-processing framework developed in our work. The code includes several Mixture and Mixture-of-Experts (MoE) perturbation models, together with utilities for data loading, evaluation, and running quick experiments.

## ðŸ“Œ Features

- Mixture model post-processing  
- Mixture-of-Experts (MoE) post-processing  
- Single-pretrained and Two-pretrained variants  
- Supports performanceâ€“fairness trade-off experiments  
- Quickstart demo for the Adult dataset  

---

## ðŸ“‚ Repository Structure

```
fair-postproc-tasks/
â”‚
â”œâ”€â”€ algorithms/                 # Core post-processing algorithms
â”‚   â”œâ”€â”€ mixture_one_pretrained.py
â”‚   â”œâ”€â”€ mixture_two_pretrained.py
â”‚   â”œâ”€â”€ moe_one_pretrained.py
â”‚   â”œâ”€â”€ moe_two_pretrained.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/                      # Helper functions
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ common.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ metrics/                    # Fairness / performance metrics
â”‚   â””â”€â”€ (add custom metrics if needed)
â”‚
â”œâ”€â”€ data/                       # Dataset interface
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ demo/                       # Quickstart examples
â”‚   â”œâ”€â”€ quickstart_adult.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ main.py                     # Main entry point for running experiments
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## ðŸ“¦ Installation

### 1. (Optional) Create a virtual environment

```bash
python3 -m venv env
source env/bin/activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸš€ Quickstart Example

Run a simple experiment using the Adult dataset:

```bash
python demo/quickstart_adult.py
```

Or run the main script with custom parameters:

```bash
python main.py --dataset adult --lambda 0.1 --model mixture_two
```

Available `--model` options:

- `mixture_one`
- `mixture_two`
- `moe_one`
- `moe_two`

---

## ðŸ§© Algorithm Summary

### Mixture Models
Use a **global scalar weight** to combine predictions from performance and fairness models.

### Mixture-of-Experts (MoE)
Use a **gating network** (e.g., logistic regression) to learn instance-specific weights.

Both support:

- **One-pretrained** version  
- **Two-pretrained** version  

---

## ðŸ“Š Datasets

